# -*- coding: utf-8 -*-
"""Practical_2_Conditioning_and_Q_value_Tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DaDIOoi609YcRlyOuZ8999MLjaB9dA5I

# **Practical 2**

In this practical, we will examine classical conditioning and use exploration/exploitation to solve a maze.

# **Conditioning vs Supervised and Online Learning**

We start by looking at the following setting. Suppose there is a dog who is in a room with two lights (one red and one green) which will be on or off depending on whether there is food available on a plate. 

We start by generating the data for training and testing. Note this is effectively an XOr gate, which we use below to obtain the targets
"""

# Import libraries and namespaces
import time
from random import seed
from random import randint
import numpy as np

x_train = np.zeros( (100, 2) )
y_train = np.zeros( 100 )
for i in range(100):
    # seed random number generator with the system clock
    seed(time.clock())
        
    # generate random integers between zero and two
    x_train[i,0] = randint(0,1)
    x_train[i,1] = randint(0,1)
    y_train[i] = np.logical_xor(x_train[i,0],x_train[i,1])
    
x_test = [[1,1],[1,0],[0,1],[0,0]]
y_test = [0,1,1,0]

# Converting to float 32bit
x_train = np.array(x_train).astype(np.float32)
x_test  = np.array(x_test).astype(np.float32)
y_train = np.array(y_train).astype(np.float32)
y_test  = np.array(y_test).astype(np.float32)

# Print data split for validation
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

"""We now try the online learning approach with an observation window, i.e. memory. In this case, the network will train with only a few samples at a time, drawing a new observation at every training step"""

# Importing libraries and namespaces
import scipy as sci
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Setup the variables
y=np.array([0]).astype(np.float32)
x=np.reshape(np.array([0,0]),(1,2)).astype(np.float32)

# Define a shift for the online learning window
def shift(A, N):
    B = np.empty_like(A)
    if N >= 0:
        B[:N] = np.nan
        B[N:] = A[:-N]
    else:
        B[N:] = np.nan
        B[:N] = A[-N:]
    return B

# Set a learning window size. This is the memory of the system
window = 10;

# Model initialization
Model = MLPClassifier(hidden_layer_sizes=(3,3), max_iter=1, alpha=0.01, #try change hidden layer
                     solver='sgd', verbose=1,  random_state=100) 

# Train our model
cl = np.array(np.unique([0,1]))
y = y_train[range(window)]
x = x_train[range(window),:]

for epochs in range(10000):
    # Sample randomly
    i = randint(0,99)
    # Train the model
    Model.partial_fit(x, y, cl)
    # Do the shift and acquire the next instance
    y = shift(y,1)
    x = shift(x,1)
    y[0] = y_train[i]
    x[0,:] = x_train[i,:]
       

#Show the accuracy
y_pred = Model.predict(x_test)
print('accuracy is ',accuracy_score(y_pred,y_test)) # Print accuracy score

"""We compare our result with that obtained using supervised learning. In the code below, we have used a very similar setting, with the same data and classifier, but with a batch rather than online sampling strategy. """

# Importing libraries and namespaces
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import mean_squared_error 
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

# Model initialization
Model = MLPClassifier(hidden_layer_sizes=(4,4), max_iter=10000, alpha=0.01,
                     solver='sgd', verbose=1,  random_state=100) 

# Train our model
h=Model.fit(x_train,y_train)

# Use our model to predict
y_pred=Model.predict(x_test)

# Scikit for machine learning reporting

print('Classification report')
print(classification_report(y_test,y_pred)) # Print summary report
print('accuracy is ',accuracy_score(y_pred,y_test)) # Print accuracy score

"""# **Action Selection**

We now turn our attention to action selection. To this end, we use a maze as the environment under consideration and proceed to install the required system dependencies

"""

# install required system dependencies
!apt-get install -y xvfb x11-utils  
!apt-get install x11-utils > /dev/null 2>&1
!pip install PyOpenGL==3.1.* \
            PyOpenGL-accelerate==3.1.* \
            gym[box2d]==0.17.* 
!pip install pyglet
!pip install ffmpeg
! pip install pyvirtualdisplay
!pip install Image
!pip install gym-maze-trustycoder83

"""If the directory vid exists and has videos left over from previous tries, its better to clean it up before continuing."""

!mkdir ./vid
!rm ./vid/*.*

"""We now proceed to initialise the monitor wrapper for Gym so we can visualise the maze and the agent on a video"""

import sys
# import pygame
import numpy as np
# import math
# import base64
# import io
# import IPython
import gym
import gym_maze

# from gym.wrappers import Monitor
# from IPython import display
from pyvirtualdisplay import Display
from gym.wrappers.monitoring import video_recorder

d = Display()
d.start()

# Recording filename
video_name = "./vid/Practical_2.mp4"

# Setup the environment for the maze
env = gym.make("maze-sample-10x10-v0")

# Setup the video
vid = None
vid = video_recorder.VideoRecorder(env,video_name)

# env = gym.wrappers.Monitor(env,'./vid',force=True)
current_state = env.reset()

"""We now proceed to perform a simple Q-tracking algorithm. The method here is quite sub-optimal since it employs a random choice to pick between exploration and exploitation. It does illustrate, however, how Q-value tracking can be done using the maze."""

discount_rate = 0.99

states_dic = {} #dictionary to keep the states/coordinates of the Q table
count = 0
for i in range(10):
    for j in range(10):
        states_dic[i, j] = count
        count+=1
        
n_actions = env.action_space.n

# Initialize the Q-table to 0
Q_table = np.zeros((len(states_dic),n_actions))


# Number of episode we will run
n_episodes = 10

# Maximum of iteration per episode
max_iter_episode = 500

# Initialize the exploration probability to 1
exploration_proba = 0.5

#Exploartion decreasing decay for exponential decreasing
exploration_decreasing_decay = 0.01

# Minimum of exploration prob
max_exploration_proba = 1
min_exploration_proba = 0.01

# Learning rate
lr = 0.1

rewards_per_episode = list()


# Iterate over episodes
for e in range(n_episodes):
    
    # We are not done yet
    done = False
    
    # Sum the rewards that the agent gets from the environment
    total_episode_reward = 0

    for i in range(max_iter_episode): 
        env.unwrapped.render()
        vid.capture_frame()
        current_coordinate_x = int(current_state[0])
        current_coordinate_y = int(current_state[1])
        current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]
        exploration_rate_threshold=np.random.uniform(0,1)
        if  exploration_rate_threshold> exploration_proba:
          action = int(np.argmax(Q_table[current_Q_table_coordinates]))
        else:
          action = env.action_space.sample()

        next_state, reward, done, _ = env.step(action)

        next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary
        next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary


        # Update our Q-table using the Q-learning iteration
        next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]
        Q_table[current_Q_table_coordinates, action] = (1-lr) *Q_table[current_Q_table_coordinates, action] +lr*(reward + max(Q_table[next_Q_table_coordinates,:]))
    
        total_episode_reward = total_episode_reward + reward
        # If the episode is finished, we leave the for loop
        if done:
            break
        current_state = next_state

    #Show the total episode reward        
    print("Total episode reward:", total_episode_reward)
    
    #Reset enviroment for next episode
    current_state = env.reset()
    #exploration rate decay
    exploration_proba = min_exploration_proba + (max_exploration_proba - min_exploration_proba) * np.exp(-exploration_decreasing_decay*e)
    rewards_per_episode.append(total_episode_reward)

    # Save video episode and close
print("Video successfuly saved.")
vid.close()
vid.enabled = False

print("\n\n********Q-table********\n")
print(Q_table)

"""We can now play the video using the following code"""

import base64
import io
from IPython import display

video_name = "./vid/Practical_2.mp4"

video = io.open(video_name, 'r+b').read()
encoded = base64.b64encode(video)

display.display(display.HTML(data="""
  <video alt="test" controls>
  <source src="data:video/mp4;base64,{0}" type="video/mp4" />
  </video>
  """.format(encoded.decode('ascii'))))

